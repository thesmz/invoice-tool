import streamlit as st
import pandas as pd
import gspread
from google.oauth2.service_account import Credentials
import unicodedata
import time
import re

st.set_page_config(page_title="Reconciliation", layout="wide", page_icon="‚öñÔ∏è")

# --- 1. CONFIGURATION ---
SKIP_KEYWORDS = [
    "ÊåØËæºÊâãÊï∞Êñô",       # Transfer Fees
    "„Ç´„Ç§„Ç¨„Ç§„ÇΩ„Ç¶„Ç≠„É≥",  # Overseas Remittance
    "JCB„Éá„Éì„ÉÉ„Éà",      # Debit Card
    "PE",             # PayEasy (Gov/Tax)
    "ÊâãÊï∞Êñô",          # Generic Fees
    "Âè£ÊåØ"            # Auto-withdrawal
]

# --- 2. AUTHENTICATION ---
if "gcp_service_account" not in st.secrets:
    st.error("Secrets not found.")
    st.stop()

creds_dict = dict(st.secrets["gcp_service_account"])

# --- 3. HELPER: UNIVERSAL TEXT NORMALIZER (Algorithmic) ---
def normalize_japanese_text(text):
    """
    ËæûÊõ∏„Çí‰Ωø„Çè„Åö„ÄÅUnicode„ÅÆ‰ªïÁµÑ„Åø„Çí‰Ωø„Å£„Å¶Ëá™ÂãïÁöÑ„Å´ÊøÅÁÇπ„ÇíÁµêÂêà„Åô„ÇãÈñ¢Êï∞
    """
    if not isinstance(text, str):
        return str(text)

    # 1. „Åæ„ÅöÊ®ôÊ∫ñÊ≠£Ë¶èÂåñ (NFKC)
    # „Åì„Çå„ÅßÂçäËßí„Ç´„Éä„ÅØÂÖ®Ëßí„Å´„Å™„Çä„ÄÅÂçäËßí„ÅÆÊøÅÁÇπ„ÅØÁµêÂêà„Åï„Çå„Åæ„Åô„ÄÇ
    # „Åó„Åã„Åó„ÄåÂÖ®Ëßí„ÅÆÂü∫Â∫ïÊñáÂ≠ó„Äç+„ÄåÂÖ®Ëßí„ÅÆÁã¨Á´ã„Åó„ÅüÊøÅÁÇπ„Äç„ÅÆÁµÑ„ÅøÂêà„Çè„Åõ„ÅØ„ÄÅ„Åì„Çå„Å†„Åë„Åß„ÅØÁµêÂêà„Åï„Çå„Åæ„Åõ„Çì„ÄÇ
    text = unicodedata.normalize('NFKC', text)
    
    # 2. Áã¨Á´ã„Åó„ÅüÊøÅÁÇπ„ÉªÂçäÊøÅÁÇπ„Çí„ÄåÁµêÂêàÁî®ÊñáÂ≠ó„Äç„Å´ÁΩÆÊèõ„Åô„Çã („Åì„Åì„Åå„Éü„ÇΩ)
    # \u309B (ÂÖ®ËßíÊøÅÁÇπ „Çõ) -> \u3099 (ÁµêÂêàÁî®ÊøÅÁÇπ)
    # \u309C (ÂÖ®ËßíÂçäÊøÅÁÇπ „Çú) -> \u309A (ÁµêÂêàÁî®ÂçäÊøÅÁÇπ)
    text = text.replace('\u309B', '\u3099').replace('\u309C', '\u309A')
    
    # 3. „ÇÇ„ÅÜ‰∏ÄÂ∫¶Ê≠£Ë¶èÂåñ (NFC)
    # ÁµêÂêàÁî®ÊñáÂ≠ó„ÅØ„ÄÅÂâç„ÅÆÊñáÂ≠ó„Å®Ëá™ÂãïÁöÑ„Å´Âêà‰Ωì„Åó„Å¶1ÊñáÂ≠ó„Å´„Å™„Çä„Åæ„Åô („Ç´ + „Çõ -> „Ç¨)
    text = unicodedata.normalize('NFC', text)
    
    # 4. Ë®òÂè∑„ÅÆÁµ±‰∏Ä („Éè„Ç§„Éï„É≥È°û„ÇíÈï∑Èü≥„Äå„Éº„Äç„Å∏)
    # ÈäÄË°å„Éá„Éº„Çø„ÅØ„Éû„Ç§„Éä„ÇπË®òÂè∑„Å™„Å©„ÅåÊ∑∑Âú®„Åó„ÇÑ„Åô„ÅÑ„Åü„ÇÅÁµ±‰∏Ä„Åó„Åæ„Åô
    text = text.replace('-', '„Éº').replace('‚àí', '„Éº').replace('‚Äê', '„Éº')
    
    # 5. ‰ΩôË®à„Å™Á©∫ÁôΩ„ÅÆÂâäÈô§
    text = text.replace('„ÄÄ', ' ').strip()
    
    return text

# --- 4. PARSER: UNIVERSAL READER ---
def parse_rakuten_file(file):
    transactions = []
    df = None
    
    # STRATEGY 1: Excel (.xlsx)
    try:
        df = pd.read_excel(file)
    except:
        pass
    
    # STRATEGY 2: CSV (UTF-8 with BOM)
    if df is None:
        try:
            file.seek(0)
            df = pd.read_csv(file, encoding='utf-8-sig')
        except:
            pass

    # STRATEGY 3: CSV (CP932 / Shift-JIS)
    if df is None:
        try:
            file.seek(0)
            df = pd.read_csv(file, encoding='cp932')
        except:
            pass

    if df is None:
        st.error("Could not read the file. Please ensure it is a valid .xlsx or .csv file.")
        return pd.DataFrame()

    # Normalize Headers
    df.columns = [str(c).strip() for c in df.columns]
    
    # Identify Columns
    date_col = next((c for c in df.columns if "ÂèñÂºïÊó•" in c), None)
    amt_col = next((c for c in df.columns if "ÂÖ•Âá∫Èáë" in c and "ÂÜÖÂÆπ" not in c), None)
    desc_col = next((c for c in df.columns if "ÂÜÖÂÆπ" in c), None)
    
    if not all([date_col, amt_col, desc_col]):
        st.error(f"Error: Columns not found. Found: {list(df.columns)}")
        return pd.DataFrame()

    # Process Rows
    for _, row in df.iterrows():
        try:
            # A. DESCRIPTION & NORMALIZE
            raw_desc = str(row[desc_col]).strip()
            norm_desc = normalize_japanese_text(raw_desc)
            
            # B. SKIP LOGIC
            if any(keyword in norm_desc for keyword in SKIP_KEYWORDS):
                continue
            
            # C. CLEAN VENDOR NAME
            # Remove (‰æùÈ†º‰∫∫...)
            clean_desc = norm_desc.split(' (‰æùÈ†º‰∫∫')[0]
            clean_desc = clean_desc.split('(‰æùÈ†º‰∫∫')[0]
            
            # Remove Bank Name prefixes
            # Logic: If 4+ spaces and starts with Bank, take the tail
            parts = clean_desc.split(' ')
            if len(parts) >= 4 and any(b in parts[0] for b in ['ÈäÄË°å', 'ÈáëÂ∫´', 'ÁµÑÂêà']):
                vendor_name = " ".join(parts[4:]) 
            else:
                vendor_name = clean_desc

            vendor_name = vendor_name.strip()

            # D. AMOUNT
            val = row[amt_col]
            if pd.isna(val): continue
            
            if isinstance(val, str):
                amount = int(float(val.replace(',', '')))
            else:
                amount = int(val)
            
            # E. DATE
            raw_date = row[date_col]
            if isinstance(raw_date, pd.Timestamp):
                date_str = raw_date.strftime("%Y/%m/%d")
            else:
                s_date = str(raw_date).replace('/', '')
                if len(s_date) == 8:
                    date_str = f"{s_date[:4]}/{s_date[4:6]}/{s_date[6:]}"
                else:
                    date_str = str(raw_date)

            # Only Keep Withdrawals
            if amount < 0:
                transactions.append({
                    "Date": date_str,
                    "Bank Description": vendor_name,
                    "Amount": abs(amount)
                })

        except Exception:
            continue
            
    return pd.DataFrame(transactions)

# --- 5. GOOGLE SHEETS HELPERS ---
def load_bank_mapping(sheet_url):
    try:
        scopes = ['https://www.googleapis.com/auth/spreadsheets']
        creds = Credentials.from_service_account_info(creds_dict, scopes=scopes)
        client = gspread.authorize(creds)
        sheet = client.open_by_url(sheet_url).worksheet("Bank Mapping")
        records = sheet.get_all_values()
        mapping = {}
        for row in records[1:]:
            if len(row) >= 2 and row[0]:
                # Keys in mapping sheet should also be normalized!
                key = normalize_japanese_text(row[0])
                mapping[key] = row[1].strip()
        return mapping
    except:
        return {}

def add_unknowns_to_sheet(sheet_url, new_names):
    try:
        scopes = ['https://www.googleapis.com/auth/spreadsheets']
        creds = Credentials.from_service_account_info(creds_dict, scopes=scopes)
        client = gspread.authorize(creds)
        sheet = client.open_by_url(sheet_url).worksheet("Bank Mapping")
        rows = [[name, ""] for name in new_names]
        sheet.append_rows(rows)
        return True
    except:
        return False

# --- 6. MAIN APP ---
st.title("‚öñÔ∏è Monthly Reconciliation")

with st.sidebar:
    st.header("‚öôÔ∏è Configuration")
    sheet_url = st.text_input("Google Sheet URL", placeholder="https://docs.google.com/spreadsheets/d/...")

if not sheet_url:
    st.info("Please enter your Google Sheet URL.")
    st.stop()

# UPLOAD
uploaded_file = st.file_uploader("1. Upload Bank File (Excel or CSV)", type=["xlsx", "csv"])

if uploaded_file:
    # Use Universal Parser
    bank_df = parse_rakuten_file(uploaded_file)
    
    if bank_df.empty:
        st.error("No valid transactions found. Please check the file.")
        st.stop()
        
    st.success(f"‚úÖ Loaded {len(bank_df)} transactions.")
    
    # Optional: Debug view to see if text is fixed
    with st.expander("üîç Check Parsed Names (Debug)"):
        st.dataframe(bank_df.head())

    # LOAD SYSTEM DATA
    try:
        scopes = ['https://www.googleapis.com/auth/spreadsheets']
        creds = Credentials.from_service_account_info(creds_dict, scopes=scopes)
        client = gspread.authorize(creds)
        sheet = client.open_by_url(sheet_url).sheet1
        sys_df = pd.DataFrame(sheet.get_all_records())
        
        status_col = next((c for c in sys_df.columns if "Status" in c), None)
        fb_col = next((c for c in sys_df.columns if "FB" in c and "Amount" in c), None)
        vendor_col = next((c for c in sys_df.columns if "Vendor" in c), None)
        
        if not all([status_col, fb_col, vendor_col]):
            st.error("Google Sheet missing required columns.")
            st.stop()
            
        paid_invoices = sys_df[sys_df[status_col] == "Paid"].copy()
    except Exception as e:
        st.error(f"Error loading Google Sheet: {e}")
        st.stop()

    # MATCHING
    mapping_dict = load_bank_mapping(sheet_url)
    matches = []
    unmatched_bank = []
    unknown_names = set()
    
    for idx, row in bank_df.iterrows():
        bank_desc = row['Bank Description']
        bank_amt = row['Amount']
        
        # Translate
        trans_name = "Unknown"
        if bank_desc in mapping_dict:
            trans_name = mapping_dict[bank_desc]
        else:
            for k, v in mapping_dict.items():
                if k in bank_desc:
                    trans_name = v
                    break
        
        if trans_name == "Unknown":
            unknown_names.add(bank_desc)
            
        # Match
        match = paid_invoices[
            (paid_invoices[vendor_col] == trans_name) & 
            (paid_invoices[fb_col] == bank_amt)
        ]
        
        if not match.empty:
            matches.append({
                "Date": row['Date'],
                "Bank Name": bank_desc,
                "System Name": trans_name,
                "Amount": f"¬•{bank_amt:,.0f}",
                "Status": "‚úÖ Match"
            })
        else:
            unmatched_bank.append({
                "Date": row['Date'],
                "Bank Name": bank_desc,
                "Translated": trans_name,
                "Amount": f"¬•{bank_amt:,.0f}",
                "Status": "‚ùå Missing"
            })

    # DISPLAY
    st.divider()
    
    if unknown_names:
        st.warning(f"Found {len(unknown_names)} unknown vendor names.")
        col_act1, col_act2 = st.columns([1, 2])
        with col_act1:
            if st.button("‚òÅÔ∏è Auto-Add Unknowns to Mapping Sheet", type="primary"):
                with st.spinner("Saving..."):
                    if add_unknowns_to_sheet(sheet_url, list(unknown_names)):
                        st.success("Added! Open 'Bank Mapping' tab to edit.")
                        time.sleep(2)
                        st.rerun()

    c1, c2 = st.columns(2)
    with c1:
        st.subheader(f"‚úÖ Matched ({len(matches)})")
        if matches:
            st.dataframe(pd.DataFrame(matches), hide_index=True, use_container_width=True)
        else:
            st.info("No matches yet.")

    with c2:
        st.subheader(f"‚ùå Unmatched ({len(unmatched_bank)})")
        if unmatched_bank:
            st.dataframe(pd.DataFrame(unmatched_bank), hide_index=True, use_container_width=True)
